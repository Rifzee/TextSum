{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading data train.01.jsonl: 100%|██████████| 14262/14262 [00:01<00:00, 8878.08it/s]\n",
      "Altering json data train.01.jsonl: 100%|██████████| 14262/14262 [00:00<00:00, 25246.17it/s]\n",
      "Loading data train.02.jsonl: 100%|██████████| 14263/14263 [00:01<00:00, 9272.13it/s]\n",
      "Altering json data train.02.jsonl: 100%|██████████| 14263/14263 [00:00<00:00, 25360.28it/s]\n",
      "Loading data train.03.jsonl: 100%|██████████| 14290/14290 [00:01<00:00, 9163.85it/s]\n",
      "Altering json data train.03.jsonl: 100%|██████████| 14290/14290 [00:00<00:00, 25123.47it/s]\n",
      "Loading data train.04.jsonl: 100%|██████████| 14272/14272 [00:01<00:00, 8862.87it/s] \n",
      "Altering json data train.04.jsonl: 100%|██████████| 14272/14272 [00:00<00:00, 24648.75it/s]\n",
      "Loading data train.05.jsonl: 100%|██████████| 14266/14266 [00:01<00:00, 8279.22it/s]\n",
      "Altering json data train.05.jsonl: 100%|██████████| 14266/14266 [00:00<00:00, 24391.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>gold_labels</th>\n",
       "      <th>id</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>source</th>\n",
       "      <th>source_url</th>\n",
       "      <th>summary</th>\n",
       "      <th>news_text</th>\n",
       "      <th>num_of_paragraphs</th>\n",
       "      <th>summary_text</th>\n",
       "      <th>num_of_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tajuk utama</td>\n",
       "      <td>{\"0\": [false, true], \"1\": [true, true], \"2\": [...</td>\n",
       "      <td>1501893029-lula-kamal-dokter-ryan-thamrin-saki...</td>\n",
       "      <td>{\"0\": [\"Jakarta , CNN Indonesia - - Dokter Rya...</td>\n",
       "      <td>cnn indonesia</td>\n",
       "      <td>https://www.cnnindonesia.com/hiburan/201708041...</td>\n",
       "      <td>{\"0\": \"Dokter Lula Kamal yang merupakan selebr...</td>\n",
       "      <td>Jakarta , CNN Indonesia - - Dokter Ryan Thamri...</td>\n",
       "      <td>9</td>\n",
       "      <td>Dokter Lula Kamal yang merupakan selebriti sek...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teknologi</td>\n",
       "      <td>{\"0\": [false, false, false, false], \"1\": [fals...</td>\n",
       "      <td>1509072914-dua-smartphone-zenfone-baru-tawarka...</td>\n",
       "      <td>{\"0\": [\"Selfie ialah salah satu tema terpanas ...</td>\n",
       "      <td>dailysocial.id</td>\n",
       "      <td>https://dailysocial.id/post/dua-smartphone-zen...</td>\n",
       "      <td>{\"0\": \"Asus memperkenalkan \\u00a0 ZenFone gene...</td>\n",
       "      <td>Selfie ialah salah satu tema terpanas di kalan...</td>\n",
       "      <td>14</td>\n",
       "      <td>Asus memperkenalkan   ZenFone generasi keempat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hiburan</td>\n",
       "      <td>{\"0\": [true], \"1\": [true], \"2\": [false, false]...</td>\n",
       "      <td>1510613677-songsong-visit-2020-bengkulu-perkua...</td>\n",
       "      <td>{\"0\": [\"Jakarta , CNN Indonesia - - Dinas Pari...</td>\n",
       "      <td>cnn indonesia</td>\n",
       "      <td>https://www.cnnindonesia.com/gaya-hidup/201711...</td>\n",
       "      <td>{\"0\": \"Dinas Pariwisata Provinsi Bengkulu kemb...</td>\n",
       "      <td>Jakarta , CNN Indonesia - - Dinas Pariwisata P...</td>\n",
       "      <td>21</td>\n",
       "      <td>Dinas Pariwisata Provinsi Bengkulu kembali men...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tajuk utama</td>\n",
       "      <td>{\"0\": [true, true], \"1\": [false, false, false]...</td>\n",
       "      <td>1502706803-icw-ada-kejanggalan-atas-tewasnya-s...</td>\n",
       "      <td>{\"0\": [\"Merdeka.com - Indonesia Corruption Wat...</td>\n",
       "      <td>merdeka</td>\n",
       "      <td>https://www.merdeka.com/peristiwa/icw-merasa-a...</td>\n",
       "      <td>{\"0\": \"Indonesia Corruption Watch ( ICW ) memi...</td>\n",
       "      <td>Merdeka.com - Indonesia Corruption Watch ( ICW...</td>\n",
       "      <td>5</td>\n",
       "      <td>Indonesia Corruption Watch ( ICW ) meminta Kom...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tajuk utama</td>\n",
       "      <td>{\"0\": [false, true], \"1\": [true, true, true], ...</td>\n",
       "      <td>1503039338-pembagian-sepeda-usai-upacara-penur...</td>\n",
       "      <td>{\"0\": [\"Merdeka.com - Presiden Joko Widodo ( J...</td>\n",
       "      <td>merdeka</td>\n",
       "      <td>https://www.merdeka.com/peristiwa/usai-upacara...</td>\n",
       "      <td>{\"0\": \"Jokowi memimpin upacara penurunan bende...</td>\n",
       "      <td>Merdeka.com - Presiden Joko Widodo ( Jokowi ) ...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jokowi memimpin upacara penurunan bendera . Us...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                        gold_labels  \\\n",
       "0  tajuk utama  {\"0\": [false, true], \"1\": [true, true], \"2\": [...   \n",
       "1    teknologi  {\"0\": [false, false, false, false], \"1\": [fals...   \n",
       "2      hiburan  {\"0\": [true], \"1\": [true], \"2\": [false, false]...   \n",
       "3  tajuk utama  {\"0\": [true, true], \"1\": [false, false, false]...   \n",
       "4  tajuk utama  {\"0\": [false, true], \"1\": [true, true, true], ...   \n",
       "\n",
       "                                                  id  \\\n",
       "0  1501893029-lula-kamal-dokter-ryan-thamrin-saki...   \n",
       "1  1509072914-dua-smartphone-zenfone-baru-tawarka...   \n",
       "2  1510613677-songsong-visit-2020-bengkulu-perkua...   \n",
       "3  1502706803-icw-ada-kejanggalan-atas-tewasnya-s...   \n",
       "4  1503039338-pembagian-sepeda-usai-upacara-penur...   \n",
       "\n",
       "                                          paragraphs          source  \\\n",
       "0  {\"0\": [\"Jakarta , CNN Indonesia - - Dokter Rya...   cnn indonesia   \n",
       "1  {\"0\": [\"Selfie ialah salah satu tema terpanas ...  dailysocial.id   \n",
       "2  {\"0\": [\"Jakarta , CNN Indonesia - - Dinas Pari...   cnn indonesia   \n",
       "3  {\"0\": [\"Merdeka.com - Indonesia Corruption Wat...         merdeka   \n",
       "4  {\"0\": [\"Merdeka.com - Presiden Joko Widodo ( J...         merdeka   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  https://www.cnnindonesia.com/hiburan/201708041...   \n",
       "1  https://dailysocial.id/post/dua-smartphone-zen...   \n",
       "2  https://www.cnnindonesia.com/gaya-hidup/201711...   \n",
       "3  https://www.merdeka.com/peristiwa/icw-merasa-a...   \n",
       "4  https://www.merdeka.com/peristiwa/usai-upacara...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  {\"0\": \"Dokter Lula Kamal yang merupakan selebr...   \n",
       "1  {\"0\": \"Asus memperkenalkan \\u00a0 ZenFone gene...   \n",
       "2  {\"0\": \"Dinas Pariwisata Provinsi Bengkulu kemb...   \n",
       "3  {\"0\": \"Indonesia Corruption Watch ( ICW ) memi...   \n",
       "4  {\"0\": \"Jokowi memimpin upacara penurunan bende...   \n",
       "\n",
       "                                           news_text  num_of_paragraphs  \\\n",
       "0  Jakarta , CNN Indonesia - - Dokter Ryan Thamri...                  9   \n",
       "1  Selfie ialah salah satu tema terpanas di kalan...                 14   \n",
       "2  Jakarta , CNN Indonesia - - Dinas Pariwisata P...                 21   \n",
       "3  Merdeka.com - Indonesia Corruption Watch ( ICW...                  5   \n",
       "4  Merdeka.com - Presiden Joko Widodo ( Jokowi ) ...                  7   \n",
       "\n",
       "                                        summary_text  num_of_summary  \n",
       "0  Dokter Lula Kamal yang merupakan selebriti sek...               3  \n",
       "1  Asus memperkenalkan   ZenFone generasi keempat...               3  \n",
       "2  Dinas Pariwisata Provinsi Bengkulu kembali men...               2  \n",
       "3  Indonesia Corruption Watch ( ICW ) meminta Kom...               2  \n",
       "4  Jokowi memimpin upacara penurunan bendera . Us...               5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Path dataset (sesuaikan dengan lokasi dataset Anda di Colab)\n",
    "DATASET_ROOT = './indosum'\n",
    "\n",
    "# Buat folder jika dataset belum ada\n",
    "if not os.path.exists(DATASET_ROOT):\n",
    "    os.makedirs(DATASET_ROOT)\n",
    "\n",
    "# Pastikan file dataset diunggah ke folder ini sebelum menjalankan kode\n",
    "files_id_dir = os.listdir(DATASET_ROOT)\n",
    "train_files = []\n",
    "\n",
    "for filename in files_id_dir:\n",
    "    if 'train' in filename:\n",
    "        train_files.append(filename)\n",
    "        \n",
    "# Fungsi untuk memuat data JSON Lines\n",
    "def load_file_to_json_list(filename):\n",
    "    file = os.path.join(DATASET_ROOT, filename)\n",
    "\n",
    "    data = []\n",
    "    with open(file, 'r') as f:\n",
    "        # Read the entire file content\n",
    "        file_content = f.read()\n",
    "        \n",
    "        # Split the content into individual JSON objects\n",
    "        json_list = file_content.splitlines() \n",
    "        \n",
    "        for json_str in tqdm(json_list, desc=f'Loading data {filename}'):\n",
    "            # Skip empty lines\n",
    "            if json_str.strip(): \n",
    "                try:\n",
    "                    d = json.loads(json_str)\n",
    "                    data.append(d)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON: {e}\")\n",
    "                    print(f\"Problematic JSON string: {json_str}\")\n",
    "                    # You might want to handle the error, e.g., skip the line or try to fix the JSON\n",
    "                    \n",
    "    return data\n",
    "\n",
    "# Fungsi untuk memproses label menjadi string JSON\n",
    "def label_to_dict_str(label_list):\n",
    "    label_dict = {}  # key = paragraph_id : value = label list \n",
    "    for i, label in enumerate(label_list[:]):\n",
    "        label_dict[i] = label\n",
    "\n",
    "    json_str = json.dumps(label_dict)\n",
    "    num = len(label_dict)\n",
    "    return json_str, num\n",
    "\n",
    "# Fungsi untuk memproses paragraph menjadi string JSON\n",
    "def paragraph_to_dict_str(paragraph_list):\n",
    "    paragraph_dict = {}  # key = paragraph_id : value = paragraph list \n",
    "    for i, paragraph in enumerate(paragraph_list):\n",
    "        new_paragraph = []\n",
    "        for sentence in paragraph:\n",
    "            sentence = ' '.join(sentence)\n",
    "            new_paragraph.append(sentence)\n",
    "        paragraph_dict[i] = new_paragraph\n",
    "\n",
    "    json_str = json.dumps(paragraph_dict)\n",
    "    num = len(paragraph_dict)\n",
    "    return json_str, num\n",
    "    \n",
    "# Fungsi untuk mengubah paragraf menjadi string teks\n",
    "def paragraph_to_text(raw_paragraph_list):\n",
    "    new_paragraph_list = []\n",
    "    for i, paragraph in enumerate(raw_paragraph_list):\n",
    "        paragraph_list = []\n",
    "        for sentence in paragraph:\n",
    "            sentence = ' '.join(sentence)\n",
    "            paragraph_list.append(sentence)\n",
    "\n",
    "        new_paragraph = ' '.join(paragraph_list)\n",
    "        new_paragraph_list.append(new_paragraph)\n",
    "\n",
    "    paragraph_str = ' '.join(new_paragraph_list)\n",
    "    return paragraph_str\n",
    "\n",
    "# Fungsi untuk memproses summary menjadi string JSON\n",
    "def summary_to_dict_str(summary_list):\n",
    "    summary_dict = {}  # key = summary_id : value = summary sentence \n",
    "    for i, summary in enumerate(summary_list):\n",
    "        summary_dict[i] = ' '.join(summary)\n",
    "\n",
    "    json_str = json.dumps(summary_dict)\n",
    "    num = len(summary_dict)\n",
    "    return json_str, num\n",
    "# Fungsi untuk mengubah summary menjadi string teks\n",
    "def summary_to_text(raw_summary_list):\n",
    "    summary_list = []\n",
    "    for i, summary in enumerate(raw_summary_list):\n",
    "        summary_list.append(' '.join(summary))\n",
    "\n",
    "    summary_str = ' '.join(summary_list)\n",
    "    return summary_str\n",
    "\n",
    "# Fungsi untuk mengubah data JSON\n",
    "def alter_json_data(json_list_data, filename=''):\n",
    "    new_json_list = []\n",
    "    for json_data in tqdm(json_list_data, desc=f'Altering json data {filename}'):\n",
    "        json_data = json_data.copy()\n",
    "        json_data['gold_labels'], _ = label_to_dict_str(json_data['gold_labels'])\n",
    "        json_data['news_text'] = paragraph_to_text(json_data['paragraphs'])\n",
    "        json_data['paragraphs'], num_paragraph = paragraph_to_dict_str(json_data['paragraphs'])\n",
    "        json_data['num_of_paragraphs'] = num_paragraph\n",
    "        json_data['summary_text'] = summary_to_text(json_data['summary'])\n",
    "        json_data['summary'], num_summary = summary_to_dict_str(json_data['summary'])\n",
    "        json_data['num_of_summary'] = num_summary\n",
    "\n",
    "        new_json_list.append(json_data)\n",
    "    \n",
    "    return new_json_list\n",
    "\n",
    "# Fungsi untuk membuat dataset dari JSON Lines\n",
    "def create_dataset(jsonl):\n",
    "    header = list(jsonl[0].keys())\n",
    "    dataset_list = []\n",
    "    for json_data in jsonl:\n",
    "        row = []\n",
    "        for h in header:\n",
    "            row.append(json_data[h])\n",
    "        dataset_list.append(row)\n",
    "    \n",
    "    return header, dataset_list\n",
    "\n",
    "# Fungsi untuk membuat dataset dari file JSON Lines\n",
    "def create_dataset_from_files(file_list):\n",
    "    df_header = None\n",
    "    dataset_list = []\n",
    "    for filename in file_list:\n",
    "        json_l = load_file_to_json_list(filename)\n",
    "        new_json_l = alter_json_data(json_l, filename)\n",
    "        header, dataset_part = create_dataset(new_json_l)\n",
    "        \n",
    "        if not df_header: df_header = header\n",
    "        dataset_list.extend(dataset_part)\n",
    "        \n",
    "    df_full = pd.DataFrame().from_records(dataset_list)\n",
    "    df_full = df_full.rename(columns=dict(enumerate(header)))\n",
    "    return df_full\n",
    "\n",
    "# Proses hanya data train\n",
    "df_train = create_dataset_from_files(train_files)\n",
    "\n",
    "# Tampilkan hasil\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [47 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 35, in __init__\n",
      "          parsed = _parse_requirement(requirement_string)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 64, in parse_requirement\n",
      "          return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 82, in _parse_requirement\n",
      "          url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 126, in _parse_requirement_details\n",
      "          marker = _parse_requirement_marker(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 147, in _parse_requirement_marker\n",
      "          tokenizer.raise_syntax_error(\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\_tokenizer.py\", line 165, in raise_syntax_error\n",
      "          raise ParserSyntaxError(\n",
      "      setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "          python_version>\"3.7\"\n",
      "                        ^\n",
      "      \n",
      "      The above exception was the direct cause of the following exception:\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Arief M\\AppData\\Local\\Temp\\pip-install-ylivx365\\tensorflow-gpu_05fab27f6f854d328bf2b5d47d223496\\setup.py\", line 40, in <module>\n",
      "          setuptools.setup()\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\__init__.py\", line 102, in setup\n",
      "          _install_setup_requires(attrs)\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\__init__.py\", line 73, in _install_setup_requires\n",
      "          dist.parse_config_files(ignore_option_errors=True)\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\_virtualenv.py\", line 22, in parse_config_files\n",
      "          result = old_parse_config_files(self, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\dist.py\", line 636, in parse_config_files\n",
      "          self._finalize_requires()\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\dist.py\", line 370, in _finalize_requires\n",
      "          self._normalize_requires()\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\dist.py\", line 385, in _normalize_requires\n",
      "          self.install_requires = list(map(str, _reqs.parse(install_requires)))\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 37, in __init__\n",
      "          raise InvalidRequirement(str(e)) from e\n",
      "      setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "          python_version>\"3.7\"\n",
      "                        ^\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load Pegasus model dan tokenizer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m pegasus_tokenizer \u001b[38;5;241m=\u001b[39m PegasusTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthonyyy/pegasus_indonesian_base-finetune\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load Pegasus model dan tokenizer\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"thonyyy/pegasus_indonesian_base-finetune\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\n",
    "    \"thonyyy/pegasus_indonesian_base-finetune\",\n",
    "    from_tf=True  # Memuat model dengan weights TensorFlow\n",
    ")\n",
    "# Gunakan GPU jika tersedia\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "pegasus_model = pegasus_model.to(device)\n",
    "\n",
    "def generate_summary_pegasus(article, max_length, tokenizer, model):\n",
    "    # Tokenisasi input teks\n",
    "    input_ids = tokenizer.encode(article, return_tensors='pt', truncation=True)\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # Hasilkan ringkasan\n",
    "    summary_ids = model.generate(input_ids,\n",
    "                                 max_length=max_length, \n",
    "                                 num_beams=4, \n",
    "                                 length_penalty=1.2, \n",
    "                                 no_repeat_ngram_size=3, \n",
    "                                 early_stopping=True)\n",
    "    \n",
    "    # Dekode token menjadi teks\n",
    "    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary_text\n",
    "\n",
    "# Tentukan panjang maksimum untuk ringkasan\n",
    "max_length = df_train['summary_text'].str.len().max()\n",
    "print(\"Max summary length:\", max_length)\n",
    "\n",
    "# Batasi jumlah data yang diproses\n",
    "max_steps = 10  # Sesuaikan dengan kebutuhan\n",
    "summary_generated = []\n",
    "\n",
    "# Iterasi untuk memproses data\n",
    "for i, row in tqdm(df_train[['id', 'news_text']].head(max_steps).iterrows(), total=max_steps):\n",
    "    sg = generate_summary_pegasus(row['news_text'], max_length, pegasus_tokenizer, pegasus_model)\n",
    "    summary_generated.append([row['id'], sg])\n",
    "\n",
    "# Konversi hasil menjadi DataFrame\n",
    "df_summary_generated = pd.DataFrame(summary_generated)\n",
    "df_summary_generated = df_summary_generated.rename(columns={0: 'id', 1: 'summary_generated'})\n",
    "\n",
    "# Gabungkan dengan dataset asli\n",
    "df_train_result = df_train.head(max_steps).merge(df_summary_generated, on='id')\n",
    "print(df_train_result.head())\n",
    "\n",
    "# Evaluasi menggunakan ROUGE\n",
    "from evaluate import load\n",
    "rouge = load('rouge')\n",
    "results = rouge.compute(\n",
    "    references=df_train_result['summary_text'].values,\n",
    "    predictions=df_train_result['summary_generated'].values)\n",
    "print(\"ROUGE scores:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
