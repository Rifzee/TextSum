{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (4.3.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data train.01.jsonl: 100%|██████████| 14262/14262 [00:01<00:00, 7311.82it/s]\n",
      "Altering json data train.01.jsonl: 100%|██████████| 14262/14262 [00:00<00:00, 23534.53it/s]\n",
      "Loading data train.02.jsonl: 100%|██████████| 14263/14263 [00:01<00:00, 7761.50it/s] \n",
      "Altering json data train.02.jsonl: 100%|██████████| 14263/14263 [00:00<00:00, 24869.09it/s]\n",
      "Loading data train.03.jsonl: 100%|██████████| 14290/14290 [00:01<00:00, 8943.40it/s] \n",
      "Altering json data train.03.jsonl: 100%|██████████| 14290/14290 [00:00<00:00, 25154.42it/s]\n",
      "Loading data train.04.jsonl: 100%|██████████| 14272/14272 [00:01<00:00, 14234.76it/s]\n",
      "Altering json data train.04.jsonl: 100%|██████████| 14272/14272 [00:00<00:00, 24599.56it/s]\n",
      "Loading data train.05.jsonl: 100%|██████████| 14266/14266 [00:01<00:00, 8248.40it/s] \n",
      "Altering json data train.05.jsonl: 100%|██████████| 14266/14266 [00:00<00:00, 24495.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>gold_labels</th>\n",
       "      <th>id</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>source</th>\n",
       "      <th>source_url</th>\n",
       "      <th>summary</th>\n",
       "      <th>news_text</th>\n",
       "      <th>num_of_paragraphs</th>\n",
       "      <th>summary_text</th>\n",
       "      <th>num_of_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tajuk utama</td>\n",
       "      <td>{\"0\": [false, true], \"1\": [true, true], \"2\": [...</td>\n",
       "      <td>1501893029-lula-kamal-dokter-ryan-thamrin-saki...</td>\n",
       "      <td>{\"0\": [\"Jakarta , CNN Indonesia - - Dokter Rya...</td>\n",
       "      <td>cnn indonesia</td>\n",
       "      <td>https://www.cnnindonesia.com/hiburan/201708041...</td>\n",
       "      <td>{\"0\": \"Dokter Lula Kamal yang merupakan selebr...</td>\n",
       "      <td>Jakarta , CNN Indonesia - - Dokter Ryan Thamri...</td>\n",
       "      <td>9</td>\n",
       "      <td>Dokter Lula Kamal yang merupakan selebriti sek...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teknologi</td>\n",
       "      <td>{\"0\": [false, false, false, false], \"1\": [fals...</td>\n",
       "      <td>1509072914-dua-smartphone-zenfone-baru-tawarka...</td>\n",
       "      <td>{\"0\": [\"Selfie ialah salah satu tema terpanas ...</td>\n",
       "      <td>dailysocial.id</td>\n",
       "      <td>https://dailysocial.id/post/dua-smartphone-zen...</td>\n",
       "      <td>{\"0\": \"Asus memperkenalkan \\u00a0 ZenFone gene...</td>\n",
       "      <td>Selfie ialah salah satu tema terpanas di kalan...</td>\n",
       "      <td>14</td>\n",
       "      <td>Asus memperkenalkan   ZenFone generasi keempat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hiburan</td>\n",
       "      <td>{\"0\": [true], \"1\": [true], \"2\": [false, false]...</td>\n",
       "      <td>1510613677-songsong-visit-2020-bengkulu-perkua...</td>\n",
       "      <td>{\"0\": [\"Jakarta , CNN Indonesia - - Dinas Pari...</td>\n",
       "      <td>cnn indonesia</td>\n",
       "      <td>https://www.cnnindonesia.com/gaya-hidup/201711...</td>\n",
       "      <td>{\"0\": \"Dinas Pariwisata Provinsi Bengkulu kemb...</td>\n",
       "      <td>Jakarta , CNN Indonesia - - Dinas Pariwisata P...</td>\n",
       "      <td>21</td>\n",
       "      <td>Dinas Pariwisata Provinsi Bengkulu kembali men...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tajuk utama</td>\n",
       "      <td>{\"0\": [true, true], \"1\": [false, false, false]...</td>\n",
       "      <td>1502706803-icw-ada-kejanggalan-atas-tewasnya-s...</td>\n",
       "      <td>{\"0\": [\"Merdeka.com - Indonesia Corruption Wat...</td>\n",
       "      <td>merdeka</td>\n",
       "      <td>https://www.merdeka.com/peristiwa/icw-merasa-a...</td>\n",
       "      <td>{\"0\": \"Indonesia Corruption Watch ( ICW ) memi...</td>\n",
       "      <td>Merdeka.com - Indonesia Corruption Watch ( ICW...</td>\n",
       "      <td>5</td>\n",
       "      <td>Indonesia Corruption Watch ( ICW ) meminta Kom...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tajuk utama</td>\n",
       "      <td>{\"0\": [false, true], \"1\": [true, true, true], ...</td>\n",
       "      <td>1503039338-pembagian-sepeda-usai-upacara-penur...</td>\n",
       "      <td>{\"0\": [\"Merdeka.com - Presiden Joko Widodo ( J...</td>\n",
       "      <td>merdeka</td>\n",
       "      <td>https://www.merdeka.com/peristiwa/usai-upacara...</td>\n",
       "      <td>{\"0\": \"Jokowi memimpin upacara penurunan bende...</td>\n",
       "      <td>Merdeka.com - Presiden Joko Widodo ( Jokowi ) ...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jokowi memimpin upacara penurunan bendera . Us...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                        gold_labels  \\\n",
       "0  tajuk utama  {\"0\": [false, true], \"1\": [true, true], \"2\": [...   \n",
       "1    teknologi  {\"0\": [false, false, false, false], \"1\": [fals...   \n",
       "2      hiburan  {\"0\": [true], \"1\": [true], \"2\": [false, false]...   \n",
       "3  tajuk utama  {\"0\": [true, true], \"1\": [false, false, false]...   \n",
       "4  tajuk utama  {\"0\": [false, true], \"1\": [true, true, true], ...   \n",
       "\n",
       "                                                  id  \\\n",
       "0  1501893029-lula-kamal-dokter-ryan-thamrin-saki...   \n",
       "1  1509072914-dua-smartphone-zenfone-baru-tawarka...   \n",
       "2  1510613677-songsong-visit-2020-bengkulu-perkua...   \n",
       "3  1502706803-icw-ada-kejanggalan-atas-tewasnya-s...   \n",
       "4  1503039338-pembagian-sepeda-usai-upacara-penur...   \n",
       "\n",
       "                                          paragraphs          source  \\\n",
       "0  {\"0\": [\"Jakarta , CNN Indonesia - - Dokter Rya...   cnn indonesia   \n",
       "1  {\"0\": [\"Selfie ialah salah satu tema terpanas ...  dailysocial.id   \n",
       "2  {\"0\": [\"Jakarta , CNN Indonesia - - Dinas Pari...   cnn indonesia   \n",
       "3  {\"0\": [\"Merdeka.com - Indonesia Corruption Wat...         merdeka   \n",
       "4  {\"0\": [\"Merdeka.com - Presiden Joko Widodo ( J...         merdeka   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  https://www.cnnindonesia.com/hiburan/201708041...   \n",
       "1  https://dailysocial.id/post/dua-smartphone-zen...   \n",
       "2  https://www.cnnindonesia.com/gaya-hidup/201711...   \n",
       "3  https://www.merdeka.com/peristiwa/icw-merasa-a...   \n",
       "4  https://www.merdeka.com/peristiwa/usai-upacara...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  {\"0\": \"Dokter Lula Kamal yang merupakan selebr...   \n",
       "1  {\"0\": \"Asus memperkenalkan \\u00a0 ZenFone gene...   \n",
       "2  {\"0\": \"Dinas Pariwisata Provinsi Bengkulu kemb...   \n",
       "3  {\"0\": \"Indonesia Corruption Watch ( ICW ) memi...   \n",
       "4  {\"0\": \"Jokowi memimpin upacara penurunan bende...   \n",
       "\n",
       "                                           news_text  num_of_paragraphs  \\\n",
       "0  Jakarta , CNN Indonesia - - Dokter Ryan Thamri...                  9   \n",
       "1  Selfie ialah salah satu tema terpanas di kalan...                 14   \n",
       "2  Jakarta , CNN Indonesia - - Dinas Pariwisata P...                 21   \n",
       "3  Merdeka.com - Indonesia Corruption Watch ( ICW...                  5   \n",
       "4  Merdeka.com - Presiden Joko Widodo ( Jokowi ) ...                  7   \n",
       "\n",
       "                                        summary_text  num_of_summary  \n",
       "0  Dokter Lula Kamal yang merupakan selebriti sek...               3  \n",
       "1  Asus memperkenalkan   ZenFone generasi keempat...               3  \n",
       "2  Dinas Pariwisata Provinsi Bengkulu kembali men...               2  \n",
       "3  Indonesia Corruption Watch ( ICW ) meminta Kom...               2  \n",
       "4  Jokowi memimpin upacara penurunan bendera . Us...               5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path dataset (sesuaikan dengan lokasi dataset Anda di Colab)\n",
    "DATASET_ROOT = './indosum'\n",
    "\n",
    "# Buat folder jika dataset belum ada\n",
    "if not os.path.exists(DATASET_ROOT):\n",
    "    os.makedirs(DATASET_ROOT)\n",
    "\n",
    "# Pastikan file dataset diunggah ke folder ini sebelum menjalankan kode\n",
    "files_id_dir = os.listdir(DATASET_ROOT)\n",
    "train_files = []\n",
    "\n",
    "for filename in files_id_dir:\n",
    "    if 'train' in filename:\n",
    "        train_files.append(filename)\n",
    "        \n",
    "# Fungsi untuk memuat data JSON Lines\n",
    "def load_file_to_json_list(filename):\n",
    "    file = os.path.join(DATASET_ROOT, filename)\n",
    "\n",
    "    data = []\n",
    "    with open(file, 'r') as f:\n",
    "        # Read the entire file content\n",
    "        file_content = f.read()\n",
    "        \n",
    "        # Split the content into individual JSON objects\n",
    "        json_list = file_content.splitlines() \n",
    "        \n",
    "        for json_str in tqdm(json_list, desc=f'Loading data {filename}'):\n",
    "            # Skip empty lines\n",
    "            if json_str.strip(): \n",
    "                try:\n",
    "                    d = json.loads(json_str)\n",
    "                    data.append(d)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON: {e}\")\n",
    "                    print(f\"Problematic JSON string: {json_str}\")\n",
    "                    # You might want to handle the error, e.g., skip the line or try to fix the JSON\n",
    "                    \n",
    "    return data\n",
    "\n",
    "# Fungsi untuk memproses label menjadi string JSON\n",
    "def label_to_dict_str(label_list):\n",
    "    label_dict = {}  # key = paragraph_id : value = label list \n",
    "    for i, label in enumerate(label_list[:]):\n",
    "        label_dict[i] = label\n",
    "\n",
    "    json_str = json.dumps(label_dict)\n",
    "    num = len(label_dict)\n",
    "    return json_str, num\n",
    "\n",
    "# Fungsi untuk memproses paragraph menjadi string JSON\n",
    "def paragraph_to_dict_str(paragraph_list):\n",
    "    paragraph_dict = {}  # key = paragraph_id : value = paragraph list \n",
    "    for i, paragraph in enumerate(paragraph_list):\n",
    "        new_paragraph = []\n",
    "        for sentence in paragraph:\n",
    "            sentence = ' '.join(sentence)\n",
    "            new_paragraph.append(sentence)\n",
    "        paragraph_dict[i] = new_paragraph\n",
    "\n",
    "    json_str = json.dumps(paragraph_dict)\n",
    "    num = len(paragraph_dict)\n",
    "    return json_str, num\n",
    "    \n",
    "# Fungsi untuk mengubah paragraf menjadi string teks\n",
    "def paragraph_to_text(raw_paragraph_list):\n",
    "    new_paragraph_list = []\n",
    "    for i, paragraph in enumerate(raw_paragraph_list):\n",
    "        paragraph_list = []\n",
    "        for sentence in paragraph:\n",
    "            sentence = ' '.join(sentence)\n",
    "            paragraph_list.append(sentence)\n",
    "\n",
    "        new_paragraph = ' '.join(paragraph_list)\n",
    "        new_paragraph_list.append(new_paragraph)\n",
    "\n",
    "    paragraph_str = ' '.join(new_paragraph_list)\n",
    "    return paragraph_str\n",
    "\n",
    "# Fungsi untuk memproses summary menjadi string JSON\n",
    "def summary_to_dict_str(summary_list):\n",
    "    summary_dict = {}  # key = summary_id : value = summary sentence \n",
    "    for i, summary in enumerate(summary_list):\n",
    "        summary_dict[i] = ' '.join(summary)\n",
    "\n",
    "    json_str = json.dumps(summary_dict)\n",
    "    num = len(summary_dict)\n",
    "    return json_str, num\n",
    "# Fungsi untuk mengubah summary menjadi string teks\n",
    "def summary_to_text(raw_summary_list):\n",
    "    summary_list = []\n",
    "    for i, summary in enumerate(raw_summary_list):\n",
    "        summary_list.append(' '.join(summary))\n",
    "\n",
    "    summary_str = ' '.join(summary_list)\n",
    "    return summary_str\n",
    "\n",
    "# Fungsi untuk mengubah data JSON\n",
    "def alter_json_data(json_list_data, filename=''):\n",
    "    new_json_list = []\n",
    "    for json_data in tqdm(json_list_data, desc=f'Altering json data {filename}'):\n",
    "        json_data = json_data.copy()\n",
    "        json_data['gold_labels'], _ = label_to_dict_str(json_data['gold_labels'])\n",
    "        json_data['news_text'] = paragraph_to_text(json_data['paragraphs'])\n",
    "        json_data['paragraphs'], num_paragraph = paragraph_to_dict_str(json_data['paragraphs'])\n",
    "        json_data['num_of_paragraphs'] = num_paragraph\n",
    "        json_data['summary_text'] = summary_to_text(json_data['summary'])\n",
    "        json_data['summary'], num_summary = summary_to_dict_str(json_data['summary'])\n",
    "        json_data['num_of_summary'] = num_summary\n",
    "\n",
    "        new_json_list.append(json_data)\n",
    "    \n",
    "    return new_json_list\n",
    "\n",
    "# Fungsi untuk membuat dataset dari JSON Lines\n",
    "def create_dataset(jsonl):\n",
    "    header = list(jsonl[0].keys())\n",
    "    dataset_list = []\n",
    "    for json_data in jsonl:\n",
    "        row = []\n",
    "        for h in header:\n",
    "            row.append(json_data[h])\n",
    "        dataset_list.append(row)\n",
    "    \n",
    "    return header, dataset_list\n",
    "\n",
    "# Fungsi untuk membuat dataset dari file JSON Lines\n",
    "def create_dataset_from_files(file_list):\n",
    "    df_header = None\n",
    "    dataset_list = []\n",
    "    for filename in file_list:\n",
    "        json_l = load_file_to_json_list(filename)\n",
    "        new_json_l = alter_json_data(json_l, filename)\n",
    "        header, dataset_part = create_dataset(new_json_l)\n",
    "        \n",
    "        if not df_header: df_header = header\n",
    "        dataset_list.extend(dataset_part)\n",
    "        \n",
    "    df_full = pd.DataFrame().from_records(dataset_list)\n",
    "    df_full = df_full.rename(columns=dict(enumerate(header)))\n",
    "    return df_full\n",
    "\n",
    "# Proses hanya data train\n",
    "df_train = create_dataset_from_files(train_files)\n",
    "\n",
    "# Tampilkan hasil\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sumy in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from sumy) (2.32.3)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from sumy) (24.6.1)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from sumy) (3.9.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.67.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests>=2.7.0->sumy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests>=2.7.0->sumy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests>=2.7.0->sumy) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install sumy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi T5\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"panggi/t5-base-indonesian-summarization-cased\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"panggi/t5-base-indonesian-summarization-cased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t5_model = t5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]c:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:657: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.2` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.810521943959986, 'rouge2': 0.7838667383762701, 'rougeL': 0.7964652822394349, 'rougeLsum': 0.7933830466176524}\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk rangkuman dengan LexRank\n",
    "def summarize_with_lexrank(article, sentence_count=10):\n",
    "    try:\n",
    "        parser = PlaintextParser.from_string(article, Tokenizer(\"indonesian\"))\n",
    "        summarizer = LexRankSummarizer()\n",
    "        summary = summarizer(parser.document, sentence_count)\n",
    "        return \" \".join([str(sentence) for sentence in summary])\n",
    "    except Exception as e:\n",
    "        return article  # Jika gagal, kembalikan artikel asli.\n",
    "\n",
    "# Fungsi untuk rangkuman dengan T5\n",
    "def summarize_with_t5(article, max_length, tokenizer, model):\n",
    "    input_ids = tokenizer.encode(article, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = input_ids.to(device)\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1,\n",
    "        length_penalty=1.2,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=10,\n",
    "        use_cache=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Gabungan LexRank + T5\n",
    "def summarize_combined(article, lexrank_sentences=7, t5_max_length=50):\n",
    "    lexrank_summary = summarize_with_lexrank(article, sentence_count=lexrank_sentences)\n",
    "    t5_summary = summarize_with_t5(lexrank_summary, max_length=t5_max_length, tokenizer=t5_tokenizer, model=t5_model)\n",
    "    return t5_summary\n",
    "\n",
    "# Iterasi untuk dataset\n",
    "max_steps = 10  # Ubah sesuai kebutuhan\n",
    "summary_generated = []\n",
    "\n",
    "for i, row in tqdm(df_train[['id', 'news_text']].head(max_steps).iterrows(), total=max_steps):\n",
    "    sg = summarize_combined(row['news_text'], lexrank_sentences=7, t5_max_length=100)\n",
    "    summary_generated.append([row['id'], sg])\n",
    "\n",
    "# Konversi hasil menjadi DataFrame\n",
    "df_summary_generated = pd.DataFrame(summary_generated, columns=['id', 'summary_generated'])\n",
    "\n",
    "# Gabungkan dengan dataset asli\n",
    "df_train_result = df_train.head(max_steps).merge(df_summary_generated, on='id')\n",
    "\n",
    "# Evaluasi dengan ROUGE\n",
    "from evaluate import load\n",
    "rouge = load('rouge')\n",
    "\n",
    "results = rouge.compute(\n",
    "    references=df_train_result['summary_text'].values,\n",
    "    predictions=df_train_result['summary_generated'].values\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from evaluate import load\n",
    "\n",
    "# Tentukan perangkat\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load tokenizer dan model\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"panggi/t5-base-indonesian-summarization-cased\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"panggi/t5-base-indonesian-summarization-cased\")\n",
    "t5_model = t5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom dataset: Index(['No.', 'News_Text', 'Summary_GPT', 'Summary_Website Berita',\n",
      "       'Summary_Ground Truth', 'Unnamed: 5'],\n",
      "      dtype='object')\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['No.', 'News_Text', 'Summary_GPT', 'Summary_Website Berita', 'Summary_Ground Truth', 'Unnamed: 5'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['No.', 'News_Text', 'Summary_GPT', 'Summary_Website Berita', 'Summary_Ground Truth', 'Unnamed: 5'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# **1. Memuat Dataset dari Excel**\n",
    "file_path = \"Dataset Artikel Berita.xlsx\"  # Ganti dengan path file Anda\n",
    "df = pd.read_excel(file_path)\n",
    "print(\"Kolom dataset:\", df.columns)\n",
    "\n",
    "# Konversi DataFrame ke dataset Hugging Face\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split dataset menjadi train dan validation set\n",
    "hf_dataset = hf_dataset.train_test_split(test_size=0.2)\n",
    "print(hf_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40/40 [00:00<00:00, 473.03 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 422.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# **2. Preprocessing Dataset**\n",
    "def preprocess_data(examples):\n",
    "    inputs = examples[\"News_Text\"]  # Pastikan nama kolom sesuai\n",
    "    targets = examples[\"Summary_Ground Truth\"]  # Pastikan nama kolom sesuai\n",
    "    model_inputs = t5_tokenizer(inputs, max_length=256, truncation=True)\n",
    "    labels = t5_tokenizer(targets, max_length=100, truncation=True).input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenisasi dataset\n",
    "tokenized_datasets = hf_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from accelerate>=0.26.0) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from accelerate>=0.26.0) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from accelerate>=0.26.0) (2.5.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (4.67.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (1.1.1)\n",
      "Requirement already satisfied: torch in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from transformers[torch]) (2.5.1+cu118)\n",
      "Requirement already satisfied: psutil in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (6.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arief m\\documents\\belajar\\semester 5\\nlp\\project1\\project1\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'accelerate\": Expected package name at the start of dependency specifier\n",
      "    'accelerate\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "! pip install 'accelerate>={ACCELERATE_MIN_VERSION}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accelerate\n",
      "Version: 1.1.1\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Version: 4.46.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: C:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show accelerate\n",
    "! pip show transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arief M\\Documents\\belajar\\Semester 5\\NLP\\Project1\\project1\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Arief M\\AppData\\Local\\Temp\\ipykernel_32184\\4168356140.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# **3. Parameter Pelatihan**\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./panggi-finetuned-model\",\n",
    "    per_device_train_batch_size=2,  # Batch kecil\n",
    "    num_train_epochs=3,  # Tambahkan epoch karena dataset kecil\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5  # Learning rate standar\n",
    ")\n",
    "# Data collator untuk padding otomatis\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model)\n",
    "\n",
    "# **4. Trainer**\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=t5_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [01:28<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0504, 'grad_norm': 0.11433077603578568, 'learning_rate': 4.25e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [01:45<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0498, 'grad_norm': 1.0343600511550903, 'learning_rate': 3.4166666666666666e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "100%|██████████| 20/20 [01:47<00:00,  1.48s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8733692169189453, 'eval_runtime': 1.2317, 'eval_samples_per_second': 8.119, 'eval_steps_per_second': 1.624, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [02:05<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0268, 'grad_norm': 1.5743052959442139, 'learning_rate': 2.5833333333333336e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [02:23<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1041, 'grad_norm': 2.114365816116333, 'learning_rate': 1.75e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                               \n",
      "\n",
      "100%|██████████| 20/20 [02:24<00:00,  1.48s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9598140716552734, 'eval_runtime': 1.2024, 'eval_samples_per_second': 8.317, 'eval_steps_per_second': 1.663, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [02:42<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1188, 'grad_norm': 2.0441184043884277, 'learning_rate': 9.166666666666666e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [02:58<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1185, 'grad_norm': 1.9061323404312134, 'learning_rate': 8.333333333333333e-07, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "                                               \n",
      "100%|██████████| 20/20 [03:11<00:00,  1.48s/it]\n",
      "\u001b[A\n",
      "100%|██████████| 60/60 [01:59<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9469668865203857, 'eval_runtime': 1.4446, 'eval_samples_per_second': 6.922, 'eval_steps_per_second': 1.384, 'epoch': 3.0}\n",
      "{'train_runtime': 119.8265, 'train_samples_per_second': 1.001, 'train_steps_per_second': 0.501, 'train_loss': 0.07805097450812658, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# **5. Mulai Pelatihan**\n",
    "trainer.train()\n",
    "\n",
    "# Simpan model hasil fine-tuning\n",
    "t5_model.save_pretrained(\"./t5-finetuned-model\")\n",
    "t5_tokenizer.save_pretrained(\"./t5-finetuned-model\")\n",
    "\n",
    "# **6. Fungsi untuk Membuat Ringkasan**\n",
    "def summarize_with_finetuned_t5(article):\n",
    "    input_ids = t5_tokenizer.encode(article, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    summary_ids = t5_model.generate(\n",
    "        input_ids,\n",
    "        max_length=150,\n",
    "        num_beams=8,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil evaluasi ROUGE: {'rouge1': 0.6230124083561563, 'rouge2': 0.5534934899246532, 'rougeL': 0.5855098638270305, 'rougeLsum': 0.5912560926723185}\n"
     ]
    }
   ],
   "source": [
    "# **7. Evaluasi dengan ROUGE**\n",
    "rouge = load('rouge')\n",
    "\n",
    "# Buat prediksi pada dataset test\n",
    "references = tokenized_datasets[\"test\"][\"Summary_Website Berita\"]\n",
    "predictions = [summarize_with_finetuned_t5(text) for text in tokenized_datasets[\"test\"][\"News_Text\"]]\n",
    "\n",
    "# Hitung metrik ROUGE\n",
    "results = rouge.compute(references=references, predictions=predictions)\n",
    "print(\"Hasil evaluasi ROUGE:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
